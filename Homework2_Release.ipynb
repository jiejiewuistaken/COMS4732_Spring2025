{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiejiewuistaken/COMS4732_Spring2025/blob/main/Homework2_Release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 2: Backpropagation\n",
        "==========\n",
        "\n",
        "> **Submission Instructions:** Before the deadline, export the completed notebook to PDF and upload it to GradeScope. The PDF should clearly show your code, and the result of running the code. Check the PDF to ensure that it is readable, the font-size is not small, and no information is cut-off. There will be no make-ups or extensions for corrupted/damaged/unreadable PDFs."
      ],
      "metadata": {
        "id": "h1l01nDajEli"
      },
      "id": "h1l01nDajEli"
    },
    {
      "cell_type": "markdown",
      "id": "ae6417a2-d5eb-4f90-a708-e16026287770",
      "metadata": {
        "id": "ae6417a2-d5eb-4f90-a708-e16026287770"
      },
      "source": [
        "**Names of Collaborators**:\n",
        "\n",
        "In this problem set, you will experiment with neural networks for image recognition. We will start with a toy neural network,\n",
        "where you will build up the pieces to implement backpropagation.  Then, we will switch to industrial strength neural\n",
        "networks that are already trained, and use them for image recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981001e4-44eb-4e75-bbe3-88118da39274",
      "metadata": {
        "id": "981001e4-44eb-4e75-bbe3-88118da39274"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c58b5b39-5b15-4194-ba86-febfb331ca90",
      "metadata": {
        "id": "c58b5b39-5b15-4194-ba86-febfb331ca90"
      },
      "source": [
        "Problem 1: Neural Network Layers\n",
        "---------------------\n",
        "\n",
        "We are going to implement a simple neural network that uses fully connected layers and the ReLU activation function. In this problem, we first implement the individual layers, then later on you will chain them together to create a neural network.\n",
        "\n",
        "The ReLU layer should follow the implementation discussed in lecture. The fully connected layer is just a linear layer without a bias term. It is called a fully connected layer because every input \"neuron\" is connected to every output \"neuron\" through a weighted summation.\n",
        "\n",
        "The input to the forward pass of each layer will always be an NxB matrix, where N is the number of dimensions for an input vector, and B is the number of vectors in the batch. Since neural networks need to perform gradient descent over many millions of iterations, we need to make each step very efficient. On most hardware, it is much faster to batch operations together. We will batch the forward pass to operate on B vectors of dimension N at once, and we will do a similar batching for the backwards pass too.\n",
        "\n",
        "Implement the following methods:\n",
        "- `forwards()` for ReLU\n",
        "- `backwards()` for ReLU\n",
        "- `forwards()` for FullyConnectedLayer\n",
        "- `backwards()` for FullyConnectedLayer\n",
        "- `backwards_param()` for FullyConnectedLayer\n",
        "- `update_param()` for FullyConnectedLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f913ede-b2c4-4494-bfeb-4d4fbc4b7bb0",
      "metadata": {
        "id": "0f913ede-b2c4-4494-bfeb-4d4fbc4b7bb0"
      },
      "outputs": [],
      "source": [
        "class ReLU(object):\n",
        "    def forwards(self, x):\n",
        "        return # TODO: Return the result for the forward function of ReLU.\n",
        "\n",
        "    def backwards(self, x, grad_output):\n",
        "        assert(x.shape == grad_output.shape)\n",
        "        grad = np.zeros(x.shape)\n",
        "        # TODO: Return the gradient with respect to the input.\n",
        "        return grad\n",
        "\n",
        "    def backwards_param(self, x, grad_output):\n",
        "        return None # We return none because there are no learnable parameters.\n",
        "\n",
        "class FullyConnectedLayer(object):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.out_dim = out_dim # During forward pass, the resulting output vector will have out_dim dimensions\n",
        "        self.in_dim = in_dim # During forward pass, each input vector will have in_dim dimensions\n",
        "\n",
        "        # Create an initial guess for the parameters w by sampling from a Gaussian\n",
        "        # distribution.\n",
        "        self.w = np.random.normal(0, math.sqrt(2 / (in_dim + out_dim)), [out_dim, in_dim])\n",
        "\n",
        "    def forwards(self, x):\n",
        "        # Computes the forward pass of a linear layer (which is also called\n",
        "        # fully connected). The input x will be a matrix that has the shape:\n",
        "        # (in_dim)x(batch_size). The output should be a matrix that has the\n",
        "        # shape (out_dim)x(batch_size). Note: in this implementation, there is\n",
        "        # no bias term in order to keep it simple.\n",
        "        assert(x.shape[0] == self.in_dim)\n",
        "        return # TODO: Return the result of the forwards pass.\n",
        "\n",
        "    def backwards_param(self, x, grad_output):\n",
        "        assert(grad_output.shape[0] == self.out_dim)\n",
        "        return # TODO: Return the gradient with respect to the parameters.\n",
        "\n",
        "    def backwards(self, x, grad_output):\n",
        "        assert(grad_output.shape[0] == self.out_dim)\n",
        "        return # TODO: Return the gradient with respect to the input.\n",
        "\n",
        "    def update_param(self, grad):\n",
        "        # Given the gradient with respect to the parameters, perform a gradient step.\n",
        "        # This function should modify self.w based on grad. You should implement\n",
        "        # the basic version of gradient descent. The function does not return anything.\n",
        "        # TODO: Implement this function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b784a20-83d1-4bda-8139-66a06715a621",
      "metadata": {
        "id": "4b784a20-83d1-4bda-8139-66a06715a621"
      },
      "source": [
        "The below code can be useful to test the forward pass of these functions. Feel free to design your own test cases too!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5370e337-c0be-4206-9047-8aa8a8d84173",
      "metadata": {
        "id": "5370e337-c0be-4206-9047-8aa8a8d84173"
      },
      "outputs": [],
      "source": [
        "def test_equality(name, actual, expected):\n",
        "    result = (np.abs(actual-expected) < 1e-7).all()\n",
        "    if result:\n",
        "        print(\"OK\\t\", name)\n",
        "    else:\n",
        "        print(\"FAIL\\t\", name)\n",
        "        print(\"Actual:\")\n",
        "        print(actual)\n",
        "        print(\"Expected:\")\n",
        "        print(expected)\n",
        "\n",
        "test_input = np.array([[10., -5., 3., 0., 2., -1.]])\n",
        "expected_output = np.array([[10., 0., 3., 0., 2., 0.]])\n",
        "actual_output = ReLU().forwards(test_input)\n",
        "test_equality(\"ReLU Forward 1\", actual_output, expected_output)\n",
        "\n",
        "test_input = np.array([[10., -5., 3., 0., 2., -1.], [3., 2., 1., 0., -1., -2.]])\n",
        "expected_output = np.array([[10., 0., 3., 0., 2., 0.], [3., 2., 1., 0., 0., 0.]])\n",
        "actual_output = ReLU().forwards(test_input)\n",
        "test_equality(\"ReLU Forward Batch\", actual_output, expected_output)\n",
        "\n",
        "layer = FullyConnectedLayer(6,2)\n",
        "layer.w[0, :] = 1\n",
        "layer.w[1, :] = 0\n",
        "test_input = np.array([[10, -5, 3, 0, 2, -1]]).T\n",
        "expected_output = np.array([[test_input.sum(), 0]]).T\n",
        "actual_output = layer.forwards(test_input)\n",
        "test_equality(\"Fully Connected Forward 1\", actual_output, expected_output)\n",
        "\n",
        "layer = FullyConnectedLayer(6,2)\n",
        "layer.w[0, :] = -1\n",
        "layer.w[1, :] = 2\n",
        "test_input = np.array([[10, -5, 3, 0, 2, -1]]).T\n",
        "expected_output = np.array([[-test_input.sum(), 2*test_input.sum()]]).T\n",
        "actual_output = layer.forwards(test_input)\n",
        "test_equality(\"Fully Connected Forward 2\", actual_output, expected_output)\n",
        "\n",
        "layer = FullyConnectedLayer(3,2)\n",
        "layer.w[0, :] = 1\n",
        "layer.w[1, :] = .5\n",
        "test_input = np.array([[1,2,3],[-4,-5,-6]]).T\n",
        "expected_output = np.array([[test_input[:, 0].sum(), 0.5*test_input[:, 0].sum()],\n",
        "                            [test_input[:, 1].sum(), 0.5*test_input[:, 1].sum()]]).T\n",
        "actual_output = layer.forwards(test_input)\n",
        "test_equality(\"Fully Connected Forward Batch\", actual_output, expected_output)\n",
        "\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff1587d-1c0f-4e22-ad09-69830aee0854",
      "metadata": {
        "id": "5ff1587d-1c0f-4e22-ad09-69830aee0854"
      },
      "source": [
        "We can also check whether the gradient of the output is implemented correctly. The `test_gradient_output` function will approximate the gradient with finite differencing, and compare it against the analytical gradient. If the gradients are calculated correctly, the two should be nearly equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b8bd46-5ac0-4069-848a-9ac3f9d870c1",
      "metadata": {
        "id": "83b8bd46-5ac0-4069-848a-9ac3f9d870c1"
      },
      "outputs": [],
      "source": [
        "def test_gradient_output(name, layer, x, epsilon=1e-7):\n",
        "    grad_approx = np.zeros(x.shape)\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i][j] += epsilon\n",
        "            fxph = layer.forwards(x)\n",
        "            x[i][j] -= 2 * epsilon\n",
        "            fxmh = layer.forwards(x)\n",
        "            x[i][j] += epsilon\n",
        "            grad_approx[i][j] = (fxph - fxmh).sum() / (2 * epsilon)\n",
        "    grad_backprop = layer.backwards(x, np.ones(layer.forwards(x).shape))\n",
        "\n",
        "    numerator = np.linalg.norm(grad_backprop - grad_approx)\n",
        "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_approx)\n",
        "    difference = numerator / (denominator)\n",
        "    if difference < 1e-7:\n",
        "        print(\"OK\\t\", name)\n",
        "    else:\n",
        "        print(\"FAIL\\t\", name, \"Difference={}\".format(difference))\n",
        "\n",
        "layer = ReLU()\n",
        "test_input = np.array([[3., -4, 2]]).T\n",
        "test_gradient_output(\"ReLU Output Gradient\", layer, test_input)\n",
        "\n",
        "layer = ReLU()\n",
        "test_input = np.array([[6, -1, -3], [4, 0.1, 2]]).T\n",
        "test_gradient_output(\"ReLU Output Gradient Batch\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(1,1)\n",
        "layer.w[:] = 2.\n",
        "test_input = np.array([[3.]]).T\n",
        "test_gradient_output(\"Fully Connected Output Gradient 1\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(6,2)\n",
        "test_input = np.array([[-1.,2.,-3.,4.,5.,6.]]).T\n",
        "test_gradient_output(\"Fully Connected Output Gradient 2\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(100,100)\n",
        "test_input = np.random.randn(100,1)\n",
        "test_gradient_output(\"Fully Connected Output Gradient 3\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(100,100)\n",
        "test_input = np.random.randn(100,50)\n",
        "test_gradient_output(\"Fully Connected Output Gradient Batch\", layer, test_input)\n",
        "\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bc886c-1730-41a0-ab64-49ad347b372a",
      "metadata": {
        "id": "e8bc886c-1730-41a0-ab64-49ad347b372a"
      },
      "source": [
        "The below will check that the gradient of the parameters is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bf43ec-07e9-4665-ae38-e79d8aea9d05",
      "metadata": {
        "id": "36bf43ec-07e9-4665-ae38-e79d8aea9d05"
      },
      "outputs": [],
      "source": [
        "def test_gradient_param(name, layer, x, epsilon=1e-7):\n",
        "    grad_approx = np.zeros(layer.w.shape)\n",
        "    for i in range(layer.w.shape[0]):\n",
        "        for j in range(layer.w.shape[1]):\n",
        "            layer.w[i][j] += epsilon\n",
        "            fxph = layer.forwards(x)\n",
        "            layer.w[i][j] -= 2 * epsilon\n",
        "            fxmh = layer.forwards(x)\n",
        "            layer.w[i][j] += epsilon\n",
        "            grad_approx[i][j] = (fxph - fxmh).sum() / (2 * epsilon)\n",
        "    grad_backprop = layer.backwards_param(x, np.ones(layer.forwards(x).shape))\n",
        "    numerator = np.linalg.norm(grad_backprop - grad_approx)\n",
        "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_approx)\n",
        "    difference = numerator / (denominator + epsilon)\n",
        "    if difference < 1e-7:\n",
        "        print(\"OK\\t\", name)\n",
        "    else:\n",
        "        print(\"FAIL\\t\", name, \"Difference={}\".format(difference))\n",
        "\n",
        "layer = FullyConnectedLayer(1,1)\n",
        "test_input = np.array([[1]]).T\n",
        "test_gradient_param(\"Fully Connected Params Gradient 1\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(6,2)\n",
        "test_input = np.array([[-1,2,-3,4,5,6]]).T\n",
        "test_gradient_param(\"Fully Connected Params Gradient 2\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(100,100)\n",
        "test_input = np.random.randn(100,1)\n",
        "test_gradient_param(\"Fully Connected Params Gradient 3\", layer, test_input)\n",
        "\n",
        "layer = FullyConnectedLayer(100,100)\n",
        "test_input = np.random.randn(100,50)\n",
        "test_gradient_param(\"Fully Connected Params Gradient Batch\", layer, test_input)\n",
        "\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73f22e9-8eef-4ae8-a15a-bef044fd9801",
      "metadata": {
        "id": "d73f22e9-8eef-4ae8-a15a-bef044fd9801"
      },
      "source": [
        "Problem 2: Loss Function\n",
        "-------------\n",
        "\n",
        "In order to train the parameters of the neural network, we need to implement a loss function that compares the prediction to the target. Implement the **squared error loss function**: $Loss(x,y) = \\frac{1}{n}||x - y||^2_2$ where $n$ is the batch size. The function should return both the loss and the gradient. It is standard practice to also divide the loss function by the batch size, which normalizes the loss value against the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4afec74-f987-4119-899e-b25928c0c99a",
      "metadata": {
        "id": "f4afec74-f987-4119-899e-b25928c0c99a"
      },
      "outputs": [],
      "source": [
        "def euclidean_loss(prediction, target):\n",
        "    assert(prediction.shape == target.shape)\n",
        "    # TODO: Implement a function that computes:\n",
        "    # - the loss (scalar)\n",
        "    # - the gradient of the loss with respect to the prediction (tensor)\n",
        "    # The function should return these two values as a tuple.\n",
        "    return loss, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e636d71-6bfb-4f70-bb5d-019a7f3f0133",
      "metadata": {
        "id": "9e636d71-6bfb-4f70-bb5d-019a7f3f0133"
      },
      "source": [
        "Let's test that the loss is implemented correctly and the gradients are accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ff7055-43bf-4b05-b2bb-b8cb02d0bb3d",
      "metadata": {
        "id": "63ff7055-43bf-4b05-b2bb-b8cb02d0bb3d"
      },
      "outputs": [],
      "source": [
        "def test_gradient_loss(name, x, target, epsilon=1e-7):\n",
        "    grad_approx = np.zeros(x.shape)\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i][j] += epsilon\n",
        "            fxph, _ = euclidean_loss(x, target)\n",
        "            x[i][j] -= 2 * epsilon\n",
        "            fxmh, _ = euclidean_loss(x, target)\n",
        "            x[i][j] += epsilon\n",
        "            grad_approx[i][j] = (fxph - fxmh).sum() / (2 * epsilon)\n",
        "    _, grad_exact = euclidean_loss(x, target)\n",
        "    numerator = np.linalg.norm(grad_exact - grad_approx)\n",
        "    denominator = np.linalg.norm(grad_exact) + np.linalg.norm(grad_approx)\n",
        "    difference = numerator / (denominator + epsilon)\n",
        "    if difference < 1e-7:\n",
        "        print(\"OK\\t\", name)\n",
        "    else:\n",
        "        print(\"FAIL\\t\", name, \"Difference={}\".format(difference))\n",
        "\n",
        "test_a = np.array([[-1., 5, -2, 0],[4.3, -10, 7.8, 8.4]])\n",
        "test_b = np.array([[1., -3, -2, 2],[8.4, 7.8, -10, 4.3]])\n",
        "loss, _ = euclidean_loss(test_a, test_a)\n",
        "test_equality(\"Euclidean Loss 1\", loss, 0)\n",
        "loss, _ = euclidean_loss(test_a, test_b)\n",
        "test_equality(\"Euclidean Loss 2\", loss, 739.3/4.)\n",
        "\n",
        "test_gradient_loss(\"Euclidean Loss Gradient 1\", test_a, test_a)\n",
        "test_gradient_loss(\"Euclidean Loss Gradient 2\", test_a, test_b)\n",
        "\n",
        "print('Done.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cec22e0-0ccf-42f7-b90b-2b7acf30c42a",
      "metadata": {
        "id": "8cec22e0-0ccf-42f7-b90b-2b7acf30c42a"
      },
      "source": [
        "Problem 3: Neural Network\n",
        "--------------\n",
        "\n",
        "In this section, we will combine the layers, the gradients, and the loss function together to train a neural network. We will optimize the parameters of all the layers with gradient descent where the gradients are calculated with back-propagation.\n",
        "\n",
        "We have implemented most of the class for you. Implement the rest of the **backwards()** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f32f90-5418-4b52-821d-5d836fac15d8",
      "metadata": {
        "id": "57f32f90-5418-4b52-821d-5d836fac15d8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.inputs = []\n",
        "        self.grad_params = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forwards(self, x):\n",
        "        self.inputs = []\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            self.inputs.append(out)\n",
        "            out = layer.forwards(out)\n",
        "        return out\n",
        "\n",
        "    def backwards(self, grad_output):\n",
        "        assert(len(self.inputs) == len(self.layers))\n",
        "        self.grad_params = [] # store gradients of the parameters for each layer\n",
        "\n",
        "        for layerid in range(len(self.layers)-1, -1, -1):\n",
        "            grad_params = # TODO: gradient wrt params for layer layerid\n",
        "            grad_output = # TODO gradient wrt output for layer layerid\n",
        "\n",
        "            if grad_params is not None: # store grad_params for update_param() below\n",
        "                self.grad_params.append((layerid, grad_params))\n",
        "        return grad_output\n",
        "\n",
        "    def update_param(self, step_size = 0.001):\n",
        "        for layerid, grad_param in self.grad_params:\n",
        "            self.layers[layerid].update_param(grad_param * step_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678625aa-611a-4ea8-905b-4b53cec1a0f9",
      "metadata": {
        "id": "678625aa-611a-4ea8-905b-4b53cec1a0f9"
      },
      "source": [
        "We can use the below code block to train the neural network. The code below first creates a 2 layer network,\n",
        "which consists of two fully connected layers, and trains it for many thousands of iterations with gradient descent to minimize the loss function.\n",
        "\n",
        "The function `sample_batch()` automatically creates training data for us. Given a two dimensional input $x \\in \\mathbb{R}^2$, the `sample_batch()` specifies the target output to be $|x0| - |x1|$ where $x0$ is the first dimension of $x$, $x1$ is the second dimension, and || is the absolute value operation.\n",
        "\n",
        "Experiment with a few different options. In our implementation, we are able to minimize the loss value to 0.0005. Turn in a PDF that contains the loss curve. For full credit, the loss curve should get near zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2605e7-1ce2-4131-aa81-7580a3622495",
      "metadata": {
        "id": "9b2605e7-1ce2-4131-aa81-7580a3622495"
      },
      "outputs": [],
      "source": [
        "nn = NeuralNetwork()\n",
        "nn.add(FullyConnectedLayer(2,50))\n",
        "nn.add(ReLU())\n",
        "nn.add(FullyConnectedLayer(50,1))\n",
        "\n",
        "def sample_batch(batch_size):\n",
        "    x = np.random.randn(2, batch_size)\n",
        "    y = np.expand_dims(np.abs(x[0, :]) - np.abs(x[1, :]), axis=0)\n",
        "    return x, y\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for iter in range(100001):\n",
        "    input_data, target = sample_batch(100)\n",
        "\n",
        "    output = nn.forwards(input_data)\n",
        "    loss, grad_loss = euclidean_loss(output, target)\n",
        "    nn.backwards(grad_loss)\n",
        "    nn.update_param(step_size=0.0001)\n",
        "\n",
        "    if iter % 10000 == 0:\n",
        "        print(f\"Iter{iter} Loss={loss}\")\n",
        "\n",
        "    loss_values.append(loss)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(loss_values, color='blue')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.xlabel('Iteration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7694df8-d883-4094-ab7a-38cf24bfdbd1",
      "metadata": {
        "id": "f7694df8-d883-4094-ab7a-38cf24bfdbd1"
      },
      "source": [
        "***Short Answer Question: The above neural network uses a two-layer network. For the above sample_batch() function, will it work with a single layer? Why or why not?***\n",
        "\n",
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbdbdb7-3212-41bf-829c-5fdd9764b188",
      "metadata": {
        "id": "dfbdbdb7-3212-41bf-829c-5fdd9764b188"
      },
      "source": [
        "***Short Answer Question: The loss curve usually goes down each iteration, but it is not guaranteed to go down. Give two reasons why.***\n",
        "\n",
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a63f25-d526-427d-8f16-81c333c87ac6",
      "metadata": {
        "id": "24a63f25-d526-427d-8f16-81c333c87ac6"
      },
      "source": [
        "Large Neural Networks\n",
        "---------------------\n",
        "\n",
        "The neural network we have implemented above is fairly simple, but it illustrates the core concepts. The most advanced neural networks today are heavily engineered to efficiently work on GPUs and carefully tuned to train in a reasonable amount of time.\n",
        "\n",
        "Before you continue, we need to do two things: attach a GPU to the Google Colab, and install some packages:\n",
        "\n",
        "- To attach a GPU, click on the **Runtime** menu, then **Change runtime type**, and select **GPU** as a hardware accelerator. The notebook will automatically attach to the new runtime.\n",
        "\n",
        "- We also need to install a few packages. Run the code below to automatically install them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install ftfy regex tqdm\n",
        "!pip --quiet install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "0u9m1gDMzbnb"
      },
      "id": "0u9m1gDMzbnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load some libraries. This will load PyTorch, which is a state-of-the-art library for deep learning. PyTorch is a very advanced version of the above simple neural network library that we built. We will also import `clip` which is a very large neural network built in PyTorch for natural images. Finally, we will also detect if there is GPU / CUDA support on your machine, and use it if possible. But if you don't have a GPU, this code will still work for you, and may just be a bit slower."
      ],
      "metadata": {
        "id": "6FD8WWFt0Dyj"
      },
      "id": "6FD8WWFt0Dyj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6cc5b01-9eab-45f1-9c17-7241637d5c4b",
      "metadata": {
        "id": "b6cc5b01-9eab-45f1-9c17-7241637d5c4b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have successfully attached a GPU to this runtime, it should output `Using device: cuda` above. If it says CPU instead, the code is not running on a GPU."
      ],
      "metadata": {
        "id": "1nYECcd-02mx"
      },
      "id": "1nYECcd-02mx"
    },
    {
      "cell_type": "markdown",
      "id": "4e1683fb-adc2-4003-81ca-c13f56db2a73",
      "metadata": {
        "id": "4e1683fb-adc2-4003-81ca-c13f56db2a73"
      },
      "source": [
        "Now, let's load an image for us to operate on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dce5d0c-483b-43ef-87ab-7b3c19613efa",
      "metadata": {
        "id": "2dce5d0c-483b-43ef-87ab-7b3c19613efa"
      },
      "outputs": [],
      "source": [
        "!wget -qN https://www.cs.columbia.edu/~vondrick/class/coms4732/hw2/Lion.jpg\n",
        "im = Image.open(\"Lion.jpg\")\n",
        "plt.imshow(im)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95afe30e-25f1-493d-ac0f-375697f59664",
      "metadata": {
        "id": "95afe30e-25f1-493d-ac0f-375697f59664"
      },
      "source": [
        "Let's load the already trained neural network, and print the neural network architecture. As you will see, this neural network will be huge consisting of hundreds of layers and different operations. However, the principles are the same as the toy neural network above. Each layer consists of a forwards() function, a backwards() function, and as backwards_param() if there are learnable parameters. They are all chained together with back-propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e158eb73-0018-4b30-b79f-b317f519f9b3",
      "metadata": {
        "id": "e158eb73-0018-4b30-b79f-b317f519f9b3"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93c7a20-7d21-4661-8d04-b2ce4fca5b02",
      "metadata": {
        "id": "f93c7a20-7d21-4661-8d04-b2ce4fca5b02"
      },
      "source": [
        "This neural network is able to associate images with natural language text. This is called a **multi-modal** method, which is a topic that we will cover in the later lectures in the course. However, for now, it is simple enough to use directly. Let's define a few categorical labels, and see which one the neural network picks for this image. (You can read more about this network [here](https://openai.com/blog/clip/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c59453-bd07-4df8-83b0-fc1f64331de3",
      "metadata": {
        "id": "b4c59453-bd07-4df8-83b0-fc1f64331de3"
      },
      "outputs": [],
      "source": [
        "def classify(im, labels):\n",
        "    image = preprocess(im).unsqueeze(0).to(device)\n",
        "    text = clip.tokenize(labels).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(image, text)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{labels[i]} = {probs[0,i]*100}%\")\n",
        "\n",
        "labels = [\"a truck\", \"a dog\", \"a cat\", \"a lion\"]\n",
        "\n",
        "classify(im, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe21c2c4-8f1c-46af-adc2-4d7152fb463f",
      "metadata": {
        "id": "fe21c2c4-8f1c-46af-adc2-4d7152fb463f"
      },
      "source": [
        "On our machine, this neural network is very confident that the image is a lion. Its second choice would be a cat, which makes sense!\n",
        "\n",
        "Problem 4: Say Cheese\n",
        "----------\n",
        "\n",
        "Let's experiment with the neural network a bit more. Can you find interesting cases where it works? and interesting cases where it does not?\n",
        "\n",
        "Use the below code to take picture with your webcam."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Take a webcam picture {display-mode: \"form\"}\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "SfrSpuDMKcZh"
      },
      "id": "SfrSpuDMKcZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "take_photo('photo.jpg')\n",
        "\n",
        "im = Image.open('photo.jpg')\n",
        "plt.imshow(im)\n",
        "plt.show()\n",
        "\n",
        "labels = [\"a person smiling\", \"a person frowning\"]\n",
        "\n",
        "classify(im, labels)"
      ],
      "metadata": {
        "id": "ord4dSILLO-i"
      },
      "id": "ord4dSILLO-i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Neural networks still have many failure modes. Experiment with different pictures from your webcam and different labels. What types of failures do you see?**\n",
        "\n",
        "Your Answer:"
      ],
      "metadata": {
        "id": "np6vRiyYLmog"
      },
      "id": "np6vRiyYLmog"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}